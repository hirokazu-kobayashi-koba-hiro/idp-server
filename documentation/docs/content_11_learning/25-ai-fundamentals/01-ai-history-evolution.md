---
sidebar_position: 1
---

# AI（人工知能）の歴史と進化

---

## 概要

人工知能（AI）は、1956年のダートマス会議から始まり、70年近い歴史を経て現在の大規模言語モデル（LLM）時代へと進化してきました。本ドキュメントでは、AI技術の変遷と重要なマイルストーンを解説します。

---

## AI研究の歴史年表

### 1950年代-1960年代: 黎明期

**1950年 - チューリングテスト提唱**
- アラン・チューリングが「Computing Machinery and Intelligence」を発表
- 「機械は考えることができるか?」という問いを提起

**1956年 - ダートマス会議**
- ジョン・マッカーシー、マービン・ミンスキーらが主催
- 「人工知能（Artificial Intelligence）」という用語が誕生
- 記号処理による問題解決アプローチ

**主な成果**:
- ELIZA（1966年）: 初期の自然言語処理プログラム
- 定理証明器、チェスプログラム

---

### 1970年代-1980年代: 第一次AIブームと冬の時代

**エキスパートシステムの台頭**
```
┌─────────────────────────────────────────┐
│     エキスパートシステムの構造           │
├─────────────────────────────────────────┤
│  知識ベース（ルール）                    │
│  ┌───────────────────────────────────┐  │
│  │ IF 症状 = "発熱" AND 咳 = "あり"   │  │
│  │ THEN 診断 = "インフルエンザの疑い" │  │
│  └───────────────────────────────────┘  │
│         ↓                               │
│  推論エンジン（ルールマッチング）         │
│         ↓                               │
│  結論出力                                │
└─────────────────────────────────────────┘
```

**代表例**:
- MYCIN（医療診断）
- DENDRAL（化学分析）

**限界**:
- ルールの手動作成が必要
- スケールしない
- 常識的推論ができない
- → **第一次AIの冬**（1980年代後半）

---

### 1990年代-2000年代: 機械学習の実用化

**統計的機械学習の台頭**

主要アルゴリズムの発展:
- サポートベクターマシン（SVM）
- ランダムフォレスト
- ベイジアンネットワーク

**重要なマイルストーン**:

**1997年 - IBM Deep Blue**
- チェス世界王者ガルリ・カスパロフを破る
- 探索アルゴリズムと評価関数の組み合わせ

**2006年 - ディープラーニングの萌芽**
- Geoffrey Hintonらが深層信念ネットワーク（DBN）を提案
- 多層ニューラルネットワークの事前学習手法

---

### 2010年代: ディープラーニング革命

#### 2012年 - ImageNet革命

**AlexNet の登場**
```
従来の画像認識              AlexNet（CNN）
エラー率: 25%     →      エラー率: 16%

┌─────────────────────────────────────────┐
│        畳み込みニューラルネットワーク     │
├─────────────────────────────────────────┤
│  入力画像                                │
│     ↓                                   │
│  畳み込み層（特徴抽出）                   │
│     ↓                                   │
│  プーリング層（ダウンサンプリング）       │
│     ↓                                   │
│  全結合層（分類）                        │
│     ↓                                   │
│  出力（カテゴリ確率）                     │
└─────────────────────────────────────────┘
```

**影響**:
- GPU活用による大規模学習の実用化
- 画像認識の精度が人間を超える

---

#### 2017年 - Transformer の登場

**論文**: "Attention is All You Need" (Vaswani et al.)

**革新的なポイント**:
```
従来のRNN/LSTM           Transformer
┌──────────────┐        ┌──────────────┐
│ 順次処理      │        │ 並列処理可能  │
│ 長期依存性弱い │   →   │ Self-Attention│
│ 学習遅い      │        │ 学習高速      │
└──────────────┘        └──────────────┘
```

**Self-Attention の仕組み**:
```
入力文: "The cat sat on the mat"

各単語が他の全単語との関連度を計算:
  The   cat   sat   on   the   mat
cat  0.1  1.0  0.3  0.2  0.1  0.6  ← "cat" と "mat" の関連度高い
```

**派生モデル**:
- **BERT**（2018年）: 双方向エンコーダー
- **GPT**（2018年）: 自己回帰型デコーダー

---

### 2020年代: 大規模言語モデル（LLM）時代

#### スケーリング則の発見

**Kaplan et al. (2020年) の研究**:
```
モデル性能 ∝ パラメータ数^0.073 × データ量^0.095 × 計算量^0.050

┌─────────────────────────────────────────┐
│      LLMのスケーリングトレンド            │
├─────────────────────────────────────────┤
│  GPT-2 (2019)      1.5B パラメータ       │
│  GPT-3 (2020)     175B パラメータ        │
│  GPT-4 (2023)     ~1.8T パラメータ?      │
│  Claude 3.5 (2024) 非公開               │
│  Gemini 1.5 (2024) 非公開               │
└─────────────────────────────────────────┘

性能向上 = より大きなモデル + より多いデータ
```

---

#### 主要LLMの進化

**2020年 - GPT-3**
- 175億パラメータ
- Few-shot学習の実証
- プロンプトエンジニアリングの重要性

**2022年 - ChatGPT（GPT-3.5）**
```
┌─────────────────────────────────────────┐
│      RLHF（人間フィードバック強化学習）   │
├─────────────────────────────────────────┤
│  1. 事前学習（大規模テキスト）            │
│       ↓                                 │
│  2. 教師あり微調整（対話データ）          │
│       ↓                                 │
│  3. 報酬モデル学習（人間評価）            │
│       ↓                                 │
│  4. PPO強化学習（最適化）                │
│       ↓                                 │
│  ChatGPT（会話特化型）                   │
└─────────────────────────────────────────┘
```

**インパクト**:
- 一般ユーザーへの普及（月間2億ユーザー達成）
- プログラミング支援の実用化
- 教育・ビジネスへの影響

---

**2023年 - GPT-4**
- マルチモーダル（画像入力対応）
- より高度な推論能力
- 医師国家試験合格レベル

**2024年 - Claude 3シリーズ（Anthropic）**
```
Haiku  : 高速・低コスト（日常タスク）
Sonnet : バランス型（汎用業務）
Opus   : 最高性能（複雑な推論）
```

**特徴**:
- Constitutional AI（安全性重視）
- 長文コンテキスト対応（200K トークン）
- コード生成の高精度化

**2024年 - Gemini 1.5（Google）**
- 100万トークンのコンテキストウィンドウ
- ネイティブマルチモーダル
- 動画理解能力

---

## AI技術の進化パターン

### パラダイムシフト

```
1950-1980   ルールベース         専門家の知識を記述
            ↓
1980-2010   機械学習             データから学習
            ↓
2010-2020   ディープラーニング   大規模データ + GPU
            ↓
2020-現在   Foundation Models   汎用AI（事前学習 + 転移学習）
```

### Foundation Model の概念

```
┌─────────────────────────────────────────────────────┐
│              Foundation Model                       │
│         （大規模事前学習モデル）                      │
├─────────────────────────────────────────────────────┤
│  事前学習: インターネット規模のテキスト               │
│  ┌─────────────────────────────────────────────┐   │
│  │  GPT-4, Claude, Gemini などの基盤モデル     │   │
│  └─────────────────────────────────────────────┘   │
│              ↓ ファインチューニング                  │
│  ┌─────┬──────┬──────┬──────┬─────────────┐       │
│  │翻訳 │要約  │コード│医療  │カスタマー    │       │
│  │     │      │生成  │診断  │サポート      │       │
│  └─────┴──────┴──────┴──────┴─────────────┘       │
│                                                     │
│  1つの大規模モデルから多様なタスクへ派生              │
└─────────────────────────────────────────────────────┘
```

---

## 現在のAI技術トレンド（2025-2026年）

### 1. マルチモーダルAI

**統合された知覚能力**:
```
┌─────────────────────────────────────────┐
│      マルチモーダルLLM                   │
├─────────────────────────────────────────┤
│  入力                                   │
│  ├─ テキスト                            │
│  ├─ 画像                                │
│  ├─ 音声                                │
│  └─ 動画                                │
│         ↓                               │
│  統合エンコーダー                        │
│         ↓                               │
│  推論エンジン                            │
│         ↓                               │
│  出力（テキスト/画像/音声）              │
└─────────────────────────────────────────┘
```

**実例**:
- GPT-4V（Vision）
- Gemini 1.5（ネイティブマルチモーダル）
- Claude 3（画像解析）

---

### 2. AI Agents（自律エージェント）

**従来のLLM vs AI Agent**:
```
従来のLLM               AI Agent
┌──────────────┐       ┌──────────────────────┐
│ 質問 → 回答   │       │ タスク → 計画 → 実行  │
│              │       │   ↓      ↓      ↓   │
│ 1回のやり取り │  →    │ ツール ツール ツール  │
│              │       │   ↓      ↓      ↓   │
│              │       │ フィードバック        │
│              │       │   ↓                  │
│              │       │ 再実行・修正          │
└──────────────┘       └──────────────────────┘
```

**AI Agentの特徴**:
- Tool使用（API呼び出し、ファイル操作など）
- 計画・実行・検証のループ
- 長期的なタスク遂行

**代表例**:
- AutoGPT
- BabyAGI
- Claude Agent SDK
- LangChain Agents

---

### 3. RAG（Retrieval-Augmented Generation）

**ハルシネーション対策の主流手法**:
```
┌─────────────────────────────────────────┐
│         RAG アーキテクチャ               │
├─────────────────────────────────────────┤
│  ユーザー質問                            │
│       ↓                                 │
│  ┌───────────────────┐                  │
│  │ ベクトル検索       │                  │
│  │（類似文書取得）    │ ← ベクトルDB     │
│  └───────────────────┘                  │
│       ↓                                 │
│  取得した文書をコンテキストに追加        │
│       ↓                                 │
│  LLM（文書に基づいた回答生成）           │
│       ↓                                 │
│  回答（ソース引用付き）                  │
└─────────────────────────────────────────┘
```

**メリット**:
- 最新情報の反映（学習データの時間制約を超える）
- 社内文書など非公開データの活用
- 回答根拠の明示

---

### 4. 小型・高効率モデル

**モデル圧縮技術**:
```
大規模モデル（100B+）  →  圧縮  →  小型モデル（7B-13B）

手法:
- 蒸留（Distillation）: 大モデルの知識を小モデルに転移
- 量子化（Quantization）: 精度を下げてサイズ削減
- プルーニング（Pruning）: 不要なパラメータ削除
```

**代表モデル**:
- Llama 3.2（7B/13B）
- Phi-3（3.8B）
- Gemma（2B/7B）

**用途**:
- エッジデバイス実行
- プライバシー保護（ローカル実行）
- コスト削減

---

## AI開発の民主化

### オープンソースエコシステム

**主要フレームワーク**:
```
┌─────────────────────────────────────────┐
│      AI開発スタック                      │
├─────────────────────────────────────────┤
│  アプリケーション層                      │
│  ├─ LangChain（オーケストレーション）    │
│  ├─ LlamaIndex（データ接続）             │
│  └─ AutoGen（マルチエージェント）        │
│                                         │
│  モデル層                                │
│  ├─ OpenAI API                          │
│  ├─ Anthropic Claude API                │
│  ├─ Hugging Face Transformers           │
│  └─ Ollama（ローカル実行）               │
│                                         │
│  インフラ層                              │
│  ├─ Pinecone/Weaviate（ベクトルDB）     │
│  ├─ LangSmith（観測性）                  │
│  └─ PromptLayer（プロンプト管理）        │
└─────────────────────────────────────────┘
```

---

### クラウドAIサービス

**主要プロバイダー**:

| プロバイダー | 主要サービス | 特徴 |
|-------------|-------------|------|
| **OpenAI** | GPT-4, DALL-E 3 | 先行者、API充実 |
| **Anthropic** | Claude 3.5 | 安全性、長文対応 |
| **Google** | Gemini 1.5, Vertex AI | マルチモーダル、GCP統合 |
| **AWS** | Bedrock | マルチモデル対応 |
| **Azure** | OpenAI Service | エンタープライズ向け |

---

## 今後の展望

### 短期的トレンド（1-2年）

1. **マルチモーダル統合の深化**
   - テキスト・画像・音声・動画の統合処理
   - リアルタイム処理の高速化

2. **AI Agent の実用化**
   - 業務自動化エージェント
   - コーディングアシスタント

3. **コスト最適化**
   - 効率的な小型モデル
   - 推論コスト削減

---

### 中長期的展望（3-5年）

1. **AGI（汎用人工知能）への接近**
   - マルチタスク対応能力の向上
   - 継続的学習能力

2. **エッジAI**
   - スマートフォン・IoTでの実行
   - プライバシー保護AI

3. **規制とガバナンス**
   - EU AI Act などの法規制
   - 倫理的AI設計

---

## まとめ

AIの歴史を振り返ると、以下のパターンが見えます:

1. **ブームと冬の繰り返し** → 現在は持続的成長フェーズ
2. **計算能力の向上** → GPUの発展がディープラーニングを可能に
3. **データの重要性** → インターネット規模のデータセット
4. **アーキテクチャ革新** → Transformer が現代LLMの基盤

**現在のAI時代の特徴**:
- Foundation Models による汎用性
- API経済の成立（誰でもAI活用可能）
- マルチモーダル・マルチタスク化
- オープンソースとクローズドソースの競争

---

## 参考リンク

- [Attention is All You Need（Transformer論文）](https://arxiv.org/abs/1706.03762)
- [Language Models are Few-Shot Learners（GPT-3論文）](https://arxiv.org/abs/2005.14165)
- [Constitutional AI（Claude技術）](https://www.anthropic.com/constitutional-ai)
- [Gemini Technical Report](https://arxiv.org/abs/2312.11805)

---

**次のステップ**:
- [02-llm-fundamentals.md](./02-llm-fundamentals.md) - LLMの基礎技術
- [03-prompt-engineering.md](./03-prompt-engineering.md) - プロンプトエンジニアリング入門
