---
sidebar_position: 2
---

# LLM（大規模言語モデル）の基礎

---

## 概要

大規模言語モデル（Large Language Model, LLM）は、膨大なテキストデータで学習されたニューラルネットワークで、人間のような自然言語理解・生成能力を持ちます。本ドキュメントでは、LLMの仕組み、主要概念、実用的な知識を解説します。

---

## LLMとは何か

### 定義

```
大規模言語モデル（LLM）= Transformer + 大規模パラメータ + 事前学習

┌─────────────────────────────────────────┐
│          LLMの基本構造                   │
├─────────────────────────────────────────┤
│  入力テキスト                            │
│      ↓                                  │
│  トークン化（文章を数値列に変換）         │
│      ↓                                  │
│  Transformerネットワーク                 │
│  （数十～数百層の深層ニューラルネット）   │
│      ↓                                  │
│  次のトークン予測                        │
│      ↓                                  │
│  出力テキスト                            │
└─────────────────────────────────────────┘
```

---

## トークン化（Tokenization）

### トークンとは

**定義**: テキストを小さな単位（トークン）に分割したもの

```
入力文: "ChatGPTは便利です"

トークン化（BPE方式）:
["Chat", "G", "PT", "は", "便利", "です"]
      ↓
トークンID変換:
[1234, 5678, 9012, 3456, 7890, 1357]
```

### トークン化アルゴリズム

**主要方式**:

| 方式 | 説明 | 使用例 |
|------|------|--------|
| **BPE** (Byte Pair Encoding) | 頻出サブワードを統合 | GPT-3, GPT-4 |
| **WordPiece** | Googleの方式 | BERT |
| **SentencePiece** | 言語非依存 | T5, LLaMA |

---

**BPEの動作例**:
```
元テキスト: "lowerlowerlowest"

ステップ1: 文字分割
["l", "o", "w", "e", "r", "l", "o", "w", "e", "r", ...]

ステップ2: 頻出ペアを統合
"lo" が頻出 → ["lo", "w", "e", "r", "lo", "w", "e", "r", ...]

ステップ3: さらに統合
"low" が頻出 → ["low", "e", "r", "low", "e", "r", ...]

最終: ["lower", "lower", "low", "est"]
```

---

### トークン数の重要性

**制約**:
```
GPT-4 Turbo:    128,000 トークン
Claude 3.5:     200,000 トークン
Gemini 1.5:   1,000,000 トークン

1トークン ≈ 0.75単語（英語）
1トークン ≈ 1-2文字（日本語）
```

**コスト計算**:
```
GPT-4 API料金（2024年時点）:
入力: $0.01 / 1,000トークン
出力: $0.03 / 1,000トークン

例: 10,000トークンの要約タスク
入力: 10,000トークン × $0.01 / 1,000 = $0.10
出力: 500トークン × $0.03 / 1,000 = $0.015
合計: $0.115
```

---

## Transformerアーキテクチャ

### Self-Attention メカニズム

**基本概念**: 各単語が他の全単語との関連性を計算

```
入力文: "The cat sat on the mat"

Self-Attention計算:
        The   cat   sat   on   the   mat
The    0.1   0.2   0.1   0.3   0.2   0.1
cat    0.2   1.0   0.3   0.1   0.1   0.7  ← "cat" は "mat" と関連
sat    0.1   0.3   1.0   0.4   0.1   0.2
on     0.3   0.1   0.4   1.0   0.5   0.3
the    0.2   0.1   0.1   0.5   1.0   0.8
mat    0.1   0.7   0.2   0.3   0.8   1.0

→ 文脈を考慮した単語表現を生成
```

---

**Attention の数式**（簡略版）:
```
Attention(Q, K, V) = softmax(Q・K^T / √d_k) × V

Q (Query):  "何を探しているか"
K (Key):    "各単語の特徴"
V (Value):  "実際の値"

例: "cat" の表現を計算するとき
Query(cat) と Key(mat) の内積が大きい
→ Value(mat) の情報を多く取り込む
```

---

### エンコーダー vs デコーダー

**Transformerの2つのモード**:

```
┌─────────────────────────────────────────┐
│        エンコーダー（BERT型）             │
├─────────────────────────────────────────┤
│  入力: "The [MASK] sat on the mat"      │
│         ↓                               │
│  双方向Attention（前後の文脈を見る）     │
│         ↓                               │
│  出力: [MASK] = "cat"                   │
│                                         │
│  用途: 分類、固有表現抽出、質問応答       │
└─────────────────────────────────────────┘

┌─────────────────────────────────────────┐
│      デコーダー（GPT型）                  │
├─────────────────────────────────────────┤
│  入力: "The cat sat on"                 │
│         ↓                               │
│  因果的Attention（過去のみ参照）         │
│         ↓                               │
│  出力: 次のトークン = "the"             │
│                                         │
│  用途: テキスト生成、チャット、翻訳       │
└─────────────────────────────────────────┘
```

---

**主要モデルの分類**:

| モデル | アーキテクチャ | 得意タスク |
|--------|---------------|-----------|
| **BERT** | エンコーダーのみ | 分類、NER、質問応答 |
| **GPT** | デコーダーのみ | テキスト生成、対話 |
| **T5** | エンコーダー-デコーダー | 翻訳、要約 |

---

## LLMの学習プロセス

### 1. 事前学習（Pre-training）

**目的**: 言語の一般的なパターンを学習

```
┌─────────────────────────────────────────┐
│         事前学習データ                   │
├─────────────────────────────────────────┤
│  ・Wikipedia（全言語）                   │
│  ・Webクロールデータ（Common Crawl）     │
│  ・書籍データ                            │
│  ・GitHub（コード）                      │
│  ・論文（arXiv等）                       │
│                                         │
│  合計: 数TB ～ 数十TB のテキスト         │
└─────────────────────────────────────────┘
         ↓
   次のトークン予測
         ↓
  Foundation Model
```

**次のトークン予測タスク**:
```
入力: "人工知能は"
正解: "様々な" （実際のデータから）
予測: "様々な" （モデルの出力）

損失関数で誤差計算 → バックプロパゲーション → パラメータ更新
```

---

### 2. ファインチューニング（Fine-tuning）

**目的**: 特定タスクへの最適化

```
事前学習モデル
      ↓
┌─────────────────────────────────────────┐
│    教師あり学習（SFT: Supervised FT）     │
├─────────────────────────────────────────┤
│  質問-回答ペア                           │
│  ┌─────────────────────────────────┐   │
│  │ Q: "Pythonでリストを反転する方法"│   │
│  │ A: "list.reverse() または       │   │
│  │     list[::-1] を使います"      │   │
│  └─────────────────────────────────┘   │
│                                         │
│  数千～数万のペアデータで学習            │
└─────────────────────────────────────────┘
      ↓
  タスク特化モデル
```

---

### 3. RLHF（人間フィードバック強化学習）

**目的**: 人間の好みに合った出力を学習

```
┌─────────────────────────────────────────┐
│     RLHF のプロセス                      │
├─────────────────────────────────────────┤
│  1️⃣ SFTモデルで複数の回答を生成          │
│                                         │
│  質問: "健康的な食事とは?"              │
│  回答A: "野菜を多く摂取し..."           │
│  回答B: "栄養バランスを考えて..."       │
│  回答C: "カロリー制限をして..."         │
│         ↓                               │
│  2️⃣ 人間が評価（ランキング）             │
│                                         │
│  回答B > 回答A > 回答C                  │
│         ↓                               │
│  3️⃣ 報酬モデル学習                       │
│                                         │
│  「回答Bのような出力に高スコア」         │
│         ↓                               │
│  4️⃣ PPO（強化学習）で最適化              │
│                                         │
│  高スコアの回答を出しやすいように        │
│  パラメータ調整                          │
└─────────────────────────────────────────┘
```

**RLHFの効果**:
- 有害コンテンツの抑制
- 回答の丁寧さ・正確性向上
- ユーザー満足度向上

---

## パラメータとモデルサイズ

### パラメータとは

**定義**: ニューラルネットワークの重み（学習で調整される数値）

```
簡単な例:
y = w1*x1 + w2*x2 + b

w1, w2, b = パラメータ（3個）

LLMの場合:
GPT-3: 1750億個のパラメータ
Claude 3: 非公開（推定数千億～数兆）
```

---

### モデルサイズの影響

**Scaling Laws（スケーリング則）**:
```
┌─────────────────────────────────────────┐
│    パラメータ数 vs 性能                  │
├─────────────────────────────────────────┤
│  性能                                   │
│   ↑                                     │
│   │         ╱                           │
│   │       ╱                             │
│   │     ╱                               │
│   │   ╱  ← GPT-4 (1.8T?)                │
│   │  ╱                                  │
│   │ ╱ ← GPT-3 (175B)                    │
│   │╱                                    │
│   └──────────────────→ パラメータ数      │
│                                         │
│  大きいほど性能向上（対数的）            │
└─────────────────────────────────────────┘
```

**トレードオフ**:

| 観点 | 小型（7B） | 中型（70B） | 大型（175B+） |
|------|-----------|------------|--------------|
| **性能** | 基本的 | 実用的 | 最高 |
| **速度** | 高速 | 中速 | 低速 |
| **コスト** | 低 | 中 | 高 |
| **メモリ** | 16GB~ | 64GB~ | 256GB~ |

---

## コンテキストウィンドウ

### 定義と重要性

**コンテキストウィンドウ**: LLMが一度に処理できるトークン数

```
┌─────────────────────────────────────────┐
│      コンテキストの範囲                  │
├─────────────────────────────────────────┤
│  ┌─────────────────────────────────┐   │
│  │ システムプロンプト               │   │
│  │ ユーザー質問                     │   │
│  │ 過去の会話履歴                   │   │  ← 全部合わせて
│  │ 追加コンテキスト（ドキュメント） │   │    128K トークン以内
│  └─────────────────────────────────┘   │
│         ↓                               │
│    LLMが参照できる範囲                   │
└─────────────────────────────────────────┘
```

---

**主要モデルのコンテキスト長**:

| モデル | コンテキスト長 | 対応文書量 |
|--------|---------------|----------|
| GPT-3.5 | 4K | 約3,000単語 |
| GPT-4 | 8K / 32K | 約24,000単語 |
| GPT-4 Turbo | 128K | 約96,000単語（書籍1冊分） |
| Claude 3.5 | 200K | 約150,000単語 |
| Gemini 1.5 | 1M | 約750,000単語（書籍10冊分） |

---

### 長文コンテキストの活用

**ユースケース**:

1. **長文要約**
   ```
   入力: 200ページの契約書
   出力: 主要条項の要約
   ```

2. **コードベース理解**
   ```
   入力: 複数ファイルのソースコード（10,000行）
   出力: アーキテクチャ解説
   ```

3. **会議録分析**
   ```
   入力: 3時間の会議書き起こし
   出力: 決定事項・アクションアイテム抽出
   ```

---

## 温度（Temperature）とサンプリング

### 温度パラメータ

**定義**: 出力のランダム性を制御

```
┌─────────────────────────────────────────┐
│       Temperature の影響                 │
├─────────────────────────────────────────┤
│  Temperature = 0                        │
│  ┌─────────────────────────────────┐   │
│  │ 最も確率が高いトークンを選択     │   │
│  │ → 決定論的（毎回同じ出力）       │   │
│  └─────────────────────────────────┘   │
│                                         │
│  Temperature = 0.7（デフォルト）        │
│  ┌─────────────────────────────────┐   │
│  │ 確率分布に従ってサンプリング     │   │
│  │ → 適度なランダム性               │   │
│  └─────────────────────────────────┘   │
│                                         │
│  Temperature = 1.5                      │
│  ┌─────────────────────────────────┐   │
│  │ 低確率のトークンも選ばれやすい   │   │
│  │ → 創造的だが不安定               │   │
│  └─────────────────────────────────┘   │
└─────────────────────────────────────────┘
```

---

**実例**:

入力: "人工知能の未来は"

```
Temperature = 0:
"人工知能の未来は、より高度な自動化と効率化が期待されます。"
（毎回同じ）

Temperature = 0.7:
"人工知能の未来は、医療や教育分野での革新が予想されます。"
"人工知能の未来は、倫理的課題への対応が重要になります。"
（実行ごとに変わる）

Temperature = 1.5:
"人工知能の未来は、量子コンピューティングと融合した
新しいパラダイムを生み出す可能性があります。"
（創造的だが不正確になりやすい）
```

---

### Top-p サンプリング（Nucleus Sampling）

**仕組み**: 累積確率が p に達するまでのトークンから選択

```
次のトークン候補:
"は"   : 40%  ┐
"が"   : 30%  │ 累積70% → Top-p=0.7 ならここまで
"も"   : 20%  ┘
"に"   : 5%   ← 除外
"を"   : 3%   ← 除外
"で"   : 2%   ← 除外

→ 低確率の不自然なトークンを排除
```

**推奨設定**:

| タスク | Temperature | Top-p |
|--------|-------------|-------|
| コード生成 | 0.2 | 0.95 |
| 技術文書作成 | 0.3 | 0.9 |
| 一般会話 | 0.7 | 0.95 |
| 創作 | 1.0 | 1.0 |

---

## ハルシネーション（幻覚）

### 定義

**ハルシネーション**: LLMが事実でない情報を生成する現象

```
┌─────────────────────────────────────────┐
│      ハルシネーションの例                │
├─────────────────────────────────────────┤
│  質問: "2023年のノーベル物理学賞受賞者は?" │
│                                         │
│  誤った回答（ハルシネーション）:         │
│  "2023年のノーベル物理学賞は、           │
│   量子コンピュータの研究で知られる       │
│   田中太郎氏が受賞しました。"            │
│                                         │
│  → 実在しない人物を創作                 │
└─────────────────────────────────────────┘
```

---

### 発生原因

1. **学習データの不足**
   - 知らないトピックでもパターンから推測

2. **確率的生成**
   - 「もっともらしい」文章を生成するが、事実確認はしない

3. **コンテキスト不足**
   - 質問が曖昧だと想像で補完

---

### 対策

**1. プロンプト設計**
```
❌ 悪い例:
"Pythonについて教えて"

✅ 良い例:
"Python 3.11の新機能について、
公式ドキュメントに記載されている内容のみを
箇条書きで説明してください。
不明な場合は「わかりません」と答えてください。"
```

**2. RAG（Retrieval-Augmented Generation）**
```
質問 → ベクトルDB検索 → 文書取得 → LLMに文書と質問を渡す
→ 文書に基づいた回答（ハルシネーション減少）
```

**3. ファクトチェック**
```
LLM回答 → 別のLLMで検証 → ソース引用要求
```

---

## エンベディング（埋め込み）

### 定義

**エンベディング**: テキストを数値ベクトルに変換したもの

```
テキスト           →      エンベディング
                          （高次元ベクトル）

"犬"              →  [0.2, 0.8, -0.3, 0.1, ...]  (1536次元)
"猫"              →  [0.3, 0.7, -0.2, 0.2, ...]
"自動車"          →  [-0.5, 0.1, 0.9, -0.4, ...]

類似度計算:
cos_similarity("犬", "猫")      = 0.85  ← 高い（似ている）
cos_similarity("犬", "自動車")  = 0.12  ← 低い（似ていない）
```

---

### ベクトル空間の性質

**意味的類似性を保持**:
```
┌─────────────────────────────────────────┐
│      2次元への縮小表現（イメージ）        │
├─────────────────────────────────────────┤
│   動物              乗り物               │
│    ┌─────┐        ┌─────┐             │
│    │ 犬  │        │ 車  │             │
│    │ 猫  │        │ 電車│             │
│    │象   │        │飛行機│            │
│    └─────┘        └─────┘             │
│                                         │
│  食べ物                                 │
│  ┌─────┐                               │
│  │リンゴ│                               │
│  │バナナ│                               │
│  └─────┘                               │
└─────────────────────────────────────────┘
```

---

### エンベディングの用途

**1. セマンティック検索**
```
ユーザー質問: "Pythonでファイルを読む方法"
      ↓（エンベディング化）
   [0.1, 0.5, ...]
      ↓（類似度計算）
┌─────────────────────────────────────┐
│ ドキュメント1: "ファイル入出力の基礎" │ 類似度 0.92
│ ドキュメント2: "Web APIの使い方"     │ 類似度 0.34
│ ドキュメント3: "Pythonファイル操作"  │ 類似度 0.88
└─────────────────────────────────────┘
      ↓
ドキュメント1, 3 を返す
```

**2. クラスタリング**
```
大量のドキュメント → エンベディング → K-means等で分類
→ トピック自動抽出
```

**3. 推薦システム**
```
ユーザーが読んだ記事のエンベディング
→ 類似記事を推薦
```

---

**主要エンベディングモデル**:

| モデル | 次元数 | 用途 |
|--------|--------|------|
| text-embedding-3-small (OpenAI) | 1536 | 汎用 |
| text-embedding-3-large (OpenAI) | 3072 | 高精度 |
| voyage-2 (Voyage AI) | 1024 | コード検索 |

---

## トークン予測の仕組み

### 自己回帰生成

**プロセス**:
```
入力: "人工知能とは"

ステップ1: "人工知能とは" → 次のトークン予測 → "、"
ステップ2: "人工知能とは、" → 次のトークン予測 → "コンピュータ"
ステップ3: "人工知能とは、コンピュータ" → 予測 → "が"
ステップ4: "人工知能とは、コンピュータが" → 予測 → "人間"
...
終了条件（句点、最大長）まで繰り返し

最終出力: "人工知能とは、コンピュータが人間のような
知的な振る舞いを実現する技術です。"
```

---

### 確率分布

**各ステップでの予測**:
```
入力: "天気が"

次のトークン候補の確率分布:
良い  : 25%  ┐
悪い  : 20%  │
いい  : 18%  │← サンプリングでどれか選択
崩れる: 15%  │
晴れ  : 10%  ┘
...   : 12%
```

---

## まとめ

### LLMの本質

```
┌─────────────────────────────────────────┐
│        LLMは何を学習しているか           │
├─────────────────────────────────────────┤
│  ✅ 言語の統計的パターン                 │
│  ✅ 世界知識（学習データ由来）           │
│  ✅ 推論パターン                         │
│                                         │
│  ❌ 真の理解（哲学的意味での）           │
│  ❌ リアルタイム知識                     │
│  ❌ 自己認識                             │
└─────────────────────────────────────────┘
```

### 実用上の鍵

1. **適切なプロンプト設計** → 性能を最大化
2. **コンテキスト管理** → トークン制限内で効果的に
3. **ハルシネーション対策** → RAG、ファクトチェック
4. **温度・サンプリング調整** → タスクに応じた設定
5. **コスト意識** → トークン数とAPI料金

---

## 次のステップ

- [03-prompt-engineering.md](./03-prompt-engineering.md) - 効果的なプロンプト設計
- [04-rag-architecture.md](./04-rag-architecture.md) - RAGの実装
- [05-ai-agents.md](./05-ai-agents.md) - AI Agent開発

---

## 参考リンク

- [OpenAI Tokenizer](https://platform.openai.com/tokenizer) - トークン化の可視化
- [Transformer論文](https://arxiv.org/abs/1706.03762) - Attention is All You Need
- [GPT-3論文](https://arxiv.org/abs/2005.14165) - Few-Shot Learners
- [Anthropic Claude技術解説](https://www.anthropic.com/research)
